% ==========================================================
% 2. Background: Neural Networks and Transformers (Korean)
% ==========================================================
\section{배경: 신경망과 트랜스포머}
\label{sec:background}

이 장에서는 신경망과 트랜스포머 모델에 대한 최소한의 배경을 정리한다.
목표는 딥러닝을 다시 가르치는 것이 아니라, 이후에 나오는 \emph{차원(annotation)이 붙은 다이어그램}을 이해하기 위해 필요한 표기법과 상위 구조를 고정하는 데 있다.

먼저 텐서와 선형 레이어를 어떻게 표현하는지 살펴본 뒤, 단순한 MLP에서 자기어텐션을 사용하는 시퀀스 모델, 그리고 표준 트랜스포머 블록으로 점차 확장해 나간다.
여기서 다루는 내용은 모두 널리 사용되는 표준 기법들이다.
우리가 하는 일은 단지, 이후의 그래프 표현을 쉽게 따라갈 수 있도록 이들을 재조직화하는 것이다.

\subsection{기본 표기와 텐서}

우리는 신경망을 텐서에 작용하는 단순한 연산자(operator)들의 합성으로 본다.
$L$개의 레이어를 가진 모델은 다음과 같이 쓸 수 있다.
\[
  X_0 \xrightarrow{f_1} X_1 \xrightarrow{f_2} \cdots \xrightarrow{f_L} X_L,
\]
여기서
\begin{itemize}
  \item $X_0$는 입력 텐서,
  \item $X_L$은 최종 출력,
  \item 각 $f_\ell$은 해당 레이어의 지역 연산(예: 행렬 곱, 비선형 함수, 정규화 레이어 등)이며, 자신만의 파라미터 $\theta_\ell$를 가진다.
\end{itemize}

이 문서 전반에서 다음과 같은 표기 규칙을 사용한다.
\begin{itemize}
  \item $\mathbf{X}$와 같이 볼드체 대문자는 텐서를 나타낸다.
  \item $W$와 같은 일반 대문자는 가중치 행렬을 나타낸다.
  \item $b$와 같은 일반 소문자는 bias 벡터를 나타낸다.
  \item 기울기는 $d\mathbf{X}$, $dW$처럼 앞에 $d$를 붙여 표기한다.
\end{itemize}

주로 사용하는 은닉 표현(hidden representation)은 다음과 같은 형태의 3차원 텐서이다.
\[
  \mathbf{X} \in \mathbb{R}^{B \times S \times D},
\]
여기서 $B$는 배치 크기(batch size), $S$는 시퀀스 길이(토큰 개수), $D$는 은닉(모델) 차원이다.

가중치 및 기타 파라미터는 다음과 같이 표현하는 경우가 많다.
\begin{itemize}
  \item 2차원 행렬: 예를 들어 $W \in \mathbb{R}^{D_{\text{in}} \times D_{\text{out}}}$,
  \item 1차원 벡터: 예를 들어 $b \in \mathbb{R}^{D_{\text{out}}}$.
\end{itemize}
이후 다이어그램에서는 엣지에 $[B, S, D]$, $[D, D_{\text{ff}}]$와 같은 텐서 모양을 직접 표시하므로, 이 규칙을 기억해 두면 도움이 된다.

기울기 표기에 대해서는 다음과 같이 정리한다.
우리는 기울기를
\[
  \mathrm{d}\theta = \frac{\partial \mathcal{L}}{\partial \theta}
\]
와 같은 미분 스타일(differential-style) 표기로 쓰며, $\nabla \theta$도 같은 양을 나타내는 축약 표기로 사용한다.
예를 들어,
\[
  \mathrm{d}\mathbf{X}
  \;=\;
  \frac{\partial \mathcal{L}}{\partial \mathbf{X}}
  \;=\;
  \nabla_{\mathbf{X}} \mathcal{L}
  \;=\;
  \nabla \mathbf{X}
\]
와 같이 본다.
즉, $d\mathbf{X}$, $\partial \mathcal{L} / \partial \mathbf{X}$, $\nabla_{\mathbf{X}} \mathcal{L}$, $\nabla \mathbf{X}$는 모두 \emph{같은 기울기 텐서}를 가리키며, 그림에서는 표기 간결성을 위해 주로 $d\mathbf{X}$를 사용한다.

\subsection{선형 레이어와 MLP 블록}

트랜스포머 블록에서 주요 연산량을 차지하는 구성 요소는 완전연결(fully-connected) 선형 레이어와, 이를 조합한 MLP(또는 FFN) 블록이다.

입력 텐서가 $\mathbf{X} \in \mathbb{R}^{B \times S \times D_{\text{in}}}$라고 하자.
가중치 $W \in \mathbb{R}^{D_{\text{in}} \times D_{\text{out}}}$, bias $b \in \mathbb{R}^{D_{\text{out}}}$를 가진 선형 레이어는 각 위치 $(b, s)$에 대해 같은 아핀 변환을 적용한다.
좌표 수준에서는 다음과 같이 쓸 수 있다.
\[
  \mathbf{Y}[b, s, :] = \mathbf{X}[b, s, :]\, W + b.
\]
이를 간단히
\[
  \mathbf{Y} = \mathrm{Linear}(\mathbf{X}; W, b)
\]
라고 쓸 수 있다.

전형적인 트랜스포머의 MLP 블록은 두 개의 선형 레이어와 중간 비선형성을 결합한 구조를 가진다.
\[
  \mathbf{X}
  \;\xrightarrow{\text{linear up}}\;
  \mathbf{Z}_{\text{up}}
  \;\xrightarrow{\phi}\;
  \mathbf{Z}_{\text{act}}
  \;\xrightarrow{\text{linear down}}\;
  \mathbf{Y},
\]
여기서
\begin{itemize}
  \item $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{B \times S \times D}$,
  \item $\mathbf{Z}_{\text{up}}, \mathbf{Z}_{\text{act}} \in \mathbb{R}^{B \times S \times D_{\text{ff}}}$,
  \item $D_{\text{ff}}$는 중간 피드포워드 차원으로, 보통 $D$보다 크다.
  \item $\phi$는 GELU, ReLU와 같은 원소별 비선형 함수이다.
\end{itemize}

이후 섹션에서는 이러한 선형 및 비선형 단계를 각각 하나의 노드로 그려, 순전파와 역전파에서 텐서가 어떻게 흐르는지 쉽게 추적할 수 있도록 한다.

\subsection{시퀀스 모델링과 자기어텐션 (Sequence Modeling and Self-Attention)}

앞 절에서는 입력을 서로 독립적인 벡터로 취급했다.
하지만 많은 실제 응용은 시간적·순서적 구조를 가진 \emph{시퀀스}를 다루어야 한다.
예를 들어:
\begin{itemize}
  \item 텍스트: 문장이나 문단을 토큰 시퀀스로 표현,
  \item 시계열: 시간에 따라 측정된 값들의 시퀀스,
  \item 오디오/비디오: 프레임들의 시퀀스.
\end{itemize}

우리는 시퀀스의 배치를 $\mathbf{X} \in \mathbb{R}^{B \times S \times D}$로 표현하며, 여기서 $S$는 시퀀스 길이이다.
모델은 같은 시퀀스 내의 서로 다른 위치들 사이의 \emph{의존성}을 포착해야 하며, 단순히 각 토큰을 독립적으로 변환하는 것만으로는 부족하다.

순환 신경망(RNN, LSTM, GRU 등)은 은닉 상태를 한 스텝씩 갱신하여 이러한 의존성을 모델링한다.
반면, 트랜스포머는 \emph{자기어텐션(self-attention)}을 이용해 각 위치가 같은 시퀀스 내의 다른 위치들을 직접 참조할 수 있게 한다.

은닉 상태 $\mathbf{X} \in \mathbb{R}^{B \times S \times D}$가 주어졌을 때, 자기어텐션은 먼저 이를 쿼리(Q), 키(K), 값(V)로 선형 변환한다.
\[
  \mathbf{Q} = \mathbf{X} W_Q, \quad
  \mathbf{K} = \mathbf{X} W_K, \quad
  \mathbf{V} = \mathbf{X} W_V,
\]
여기서 $W_Q, W_K, W_V \in \mathbb{R}^{D \times D}$ 또는 일반적으로 $D$에서 어떤 헤드 차원으로 가는 행렬이다.
이후 쿼리와 키의 스케일된 내적을 계산하고, softmax를 적용해 attention weight를 얻은 다음, 이를 값 벡터들의 가중합으로 사용한다.

멀티헤드 어텐션(Multi-Head Attention, MHA)에서는 $N_H$개의 헤드를 병렬로 사용하며, 각 헤드는 $D_h$ 차원을 가진다.
보통
\[
  D = N_H \cdot D_h
\]
가 되도록 설계한다.
개념적으로 주요 텐서의 모양은 다음과 같다.
\begin{itemize}
  \item 입력/출력 은닉 상태: $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{B \times S \times D}$,
  \item 헤드별 Q/K/V: $\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{B \times N_H \times S \times D_h}$,
  \item 어텐션 스코어: $\mathbf{A} \in \mathbb{R}^{B \times N_H \times S \times S}$.
\end{itemize}
이후 나오는 다이어그램에서는 이러한 모양을 명시적으로 표시하고, 순전파와 역전파에서 큰 행렬 곱이 정확히 어디에서 발생하는지 보여준다.

\subsection{표준 트랜스포머 블록 구조}

표준 트랜스포머 블록은 자기어텐션, MLP 블록, 정규화, 잔차 연결을 고정된 패턴으로 결합한다.
문헌에는 pre-norm과 post-norm처럼 몇 가지 변형이 존재하지만, 이 문서에서는 전형적인 \emph{pre-norm} 형태를 가정한다.

\begin{enumerate}
  \item \textbf{입력 및 레이어 정규화}
        블록은 입력 $\mathbf{X}_{\text{in}} \in \mathbb{R}^{B \times S \times D}$를 받아 레이어 정규화(layer normalization)를 적용한다.
        \[
          \mathbf{X}_1 = \mathrm{LayerNorm}(\mathbf{X}_{\text{in}}).
        \]
  \item \textbf{멀티헤드 자기어텐션 (MHA)}
        자기어텐션을 적용하여 $\mathbf{H}_{\text{att}} \in \mathbb{R}^{B \times S \times D}$를 얻으며, 보통 드롭아웃이 포함된다.
        \[
          \mathbf{H}_{\text{att}} = \mathrm{MHA}(\mathbf{X}_1).
        \]
  \item \textbf{첫 번째 잔차 연결}
        \[
          \mathbf{X}_2 = \mathbf{X}_{\text{in}} + \mathbf{H}_{\text{att}}.
        \]
  \item \textbf{두 번째 레이어 정규화}
        \[
          \mathbf{X}_3 = \mathrm{LayerNorm}(\mathbf{X}_2).
        \]
  \item \textbf{MLP (피드포워드) 블록}
        \[
          \mathbf{H}_{\text{mlp}} = \mathrm{MLP}(\mathbf{X}_3),
        \]
        여기서 MLP는 앞 절에서 설명한 구조를 따른다.
  \item \textbf{두 번째 잔차 연결}
        \[
          \mathbf{X}_{\text{out}} = \mathbf{X}_2 + \mathbf{H}_{\text{mlp}}.
        \]
\end{enumerate}

드롭아웃, bias, 위치 임베딩(positional embedding) 등 추가 요소가 들어갈 수 있지만, 이 문서의 \emph{차원 표기가 붙은 계산 그래프} 관점에서 중요한 것은 다음과 같다.
\begin{itemize}
  \item $[B, S, D]$ 형태의 텐서를 반복적으로 사용한다는 점,
  \item MHA 안에서 $[N_H, D_h]$ 및 $[S, S]$ 차원을 포함하는 텐서들이 등장한다는 점,
  \item $[D, D_{\text{ff}}]$, $[D_{\text{ff}}, D]$, $[D, D]$와 같은 모양의 선형 레이어 가중치가 여러 번 등장한다는 점,
  \item 은닉 차원에 대해 잔차 덧셈과 레이어 정규화가 수행된다는 점.
\end{itemize}

또한 우리는 다음과 같은 구성 요소를 전제로 한다.
\begin{itemize}
  \item 토큰 ID나 입력 특성을 $[B, S, D]$ 은닉 표현으로 사상하는 입력 임베딩(및 필요 시 위치 임베딩),
  \item 은닉 상태를 어휘 logits 또는 태스크별 출력으로 변환하는 최종 출력 프로젝션.
\end{itemize}
이들 역시 이후 다이어그램에서는 추가적인 선형 레이어로 취급한다.

\subsection{이후 내용과의 연결 (Connection to the Rest of the Document)}

이 장의 목적은 표준 트랜스포머 레이어가 어떤 형태를 가지며, 그 안에 어떤 텐서 모양과 연산자들이 등장하는지 정리하는 데 있다.
여기서 다룬 내용은 모두 표준적이고 널리 쓰이는 기법이며, 새로운 모델을 제안하지 않는다.

다음 장부터는 이러한 상위 아키텍처 관점에서 한 걸음 내려와, 보다 \emph{연산자 중심(operator-centric)} 관점으로 전환한다.
\begin{itemize}
  \item 각 순전파 연산(예: 행렬 곱, softmax, 레이어 정규화, 잔차 덧셈)을 계산 그래프의 노드로 표현하고,
  \item 이에 대응하는 역전파 연산자를 도입하여, 상류 기울기를 받아 입력 및 파라미터에 대한 기울기를 계산하는 방식으로 본다.
  \item 이 틀을 단일 노드 트랜스포머 레이어에 적용한 뒤, 텐서 병렬화, 데이터 병렬화, 그리고 이 둘의 결합으로 확장한다.
\end{itemize}

이후의 내용은 이 장에서 정의한 표준 트랜스포머 블록을, 보다 세밀한 연산 수준에서 \emph{시각적으로 풀어 쓴 것}으로 이해할 수 있다.
