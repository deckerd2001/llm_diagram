\contentsline {section}{\numberline {1}Neural Network Basics}{3}{}%
\contentsline {subsection}{\numberline {1.1}What is a Neural Network?}{3}{}%
\contentsline {subsection}{\numberline {1.2}Fully-Connected Layers and MLPs}{3}{}%
\contentsline {subsection}{\numberline {1.3}Backpropagation at a Glance}{3}{}%
\contentsline {section}{\numberline {2}From Neural Networks to Transformers}{4}{}%
\contentsline {subsection}{\numberline {2.1}Sequence Modeling Motivation}{4}{}%
\contentsline {subsection}{\numberline {2.2}The Self-Attention Idea}{4}{}%
\contentsline {section}{\numberline {3}Gradients and Backpropagation Basics}{5}{}%
\contentsline {subsection}{\numberline {3.1}Loss Functions and Gradients}{5}{}%
\contentsline {subsection}{\numberline {3.2}Chain Rule and Backward Operators}{5}{}%
\contentsline {paragraph}{Multiple inputs.}{5}{}%
\contentsline {subsection}{\numberline {3.3}Computation Graph View}{6}{}%
\contentsline {subsection}{\numberline {3.4}Parameter Gradients and Updates}{6}{}%
\contentsline {section}{\numberline {4}Graphical Notation and Figure Conventions}{6}{}%
\contentsline {subsection}{\numberline {4.1}Graph View: Nodes, Edges, and Shapes}{6}{}%
\contentsline {subsection}{\numberline {4.2}Tensor Shapes and Indices}{7}{}%
\contentsline {subsection}{\numberline {4.3}Operator Dictionary: Forward and Backward}{7}{}%
\contentsline {subsubsection}{\numberline {4.3.1}Matrix Multiplication (Matmul)}{7}{}%
\contentsline {paragraph}{Symbol}{7}{}%
\contentsline {paragraph}{Forward}{8}{}%
\contentsline {paragraph}{Backward}{8}{}%
\contentsline {subsubsection}{\numberline {4.3.2}Bitwise / Elementwise Addition}{8}{}%
\contentsline {paragraph}{Symbol}{8}{}%
\contentsline {paragraph}{Forward}{8}{}%
\contentsline {paragraph}{Backward}{8}{}%
\contentsline {subsubsection}{\numberline {4.3.3}Nonlinear and Pointwise Operations (GLU / GELU / ReLU / DO)}{8}{}%
\contentsline {paragraph}{Symbol}{8}{}%
\contentsline {paragraph}{Forward}{8}{}%
\contentsline {paragraph}{Backward (dGL, dGELU, dReLU, dDO)}{8}{}%
\contentsline {subsubsection}{\numberline {4.3.4}Scale/Mask Node (SM)}{9}{}%
\contentsline {paragraph}{Symbol and Labels}{9}{}%
\contentsline {paragraph}{Forward (SM)}{9}{}%
\contentsline {paragraph}{Backward (dSM)}{9}{}%
\contentsline {subsubsection}{\numberline {4.3.5}Softmax Node (S)}{9}{}%
\contentsline {paragraph}{Symbol and Labels}{9}{}%
\contentsline {paragraph}{Forward (S)}{9}{}%
\contentsline {paragraph}{Backward (dS)}{9}{}%
\contentsline {subsubsection}{\numberline {4.3.6}Layer Normalization (LN, dLN)}{9}{}%
\contentsline {paragraph}{Symbols and Labels}{9}{}%
\contentsline {paragraph}{Forward (LN)}{10}{}%
\contentsline {paragraph}{Backward (dLN)}{10}{}%
\contentsline {subsubsection}{\numberline {4.3.7}Reshape and Transpose (R, T)}{10}{}%
\contentsline {paragraph}{Symbols}{10}{}%
\contentsline {paragraph}{Forward}{10}{}%
\contentsline {paragraph}{Backward}{10}{}%
\contentsline {subsubsection}{\numberline {4.3.8}Broadcast (BC$_{B,S}(b_0)$)}{10}{}%
\contentsline {paragraph}{Notation and Role}{10}{}%
\contentsline {paragraph}{Forward}{10}{}%
\contentsline {paragraph}{Backward}{10}{}%
\contentsline {subsubsection}{\numberline {4.3.9}Communication Nodes (AR, AG)}{11}{}%
\contentsline {paragraph}{Symbols}{11}{}%
\contentsline {paragraph}{Forward}{11}{}%
\contentsline {paragraph}{Backward}{11}{}%
\contentsline {subsection}{\numberline {4.4}Arrow Styles}{11}{}%
\contentsline {section}{\numberline {5}Single-Node Transformer: Forward and Backward}{12}{}%
\contentsline {subsection}{\numberline {5.1}Overall Transformer Layer Flow}{12}{}%
\contentsline {subsection}{\numberline {5.2}Input Embedding Layer}{13}{}%
\contentsline {subsubsection}{\numberline {5.2.1}Forward Pass}{13}{}%
\contentsline {subsection}{\numberline {5.3}Multi-Head Attention (MHA)}{14}{}%
\contentsline {subsubsection}{\numberline {5.3.1}Forward Pass}{14}{}%
\contentsline {subsubsection}{\numberline {5.3.2}Backward Pass}{16}{}%
\contentsline {subsection}{\numberline {5.4}Feed-Forward Network (MLP / FFN)}{18}{}%
\contentsline {subsubsection}{\numberline {5.4.1}Forward Pass}{18}{}%
\contentsline {subsubsection}{\numberline {5.4.2}Backward Pass}{19}{}%
\contentsline {subsection}{\numberline {5.5}Output Projection and Loss}{20}{}%
\contentsline {subsubsection}{\numberline {5.5.1}Forward Pass (Logits, Softmax, Loss)}{20}{}%
\contentsline {subsubsection}{\numberline {5.5.2}Backward Pass}{21}{}%
\contentsline {section}{\numberline {6}Tensor Parallelism (TP)}{22}{}%
\contentsline {subsection}{\numberline {6.1}TP Overview and End-to-End Flow}{22}{}%
\contentsline {subsection}{\numberline {6.2}MHA with Tensor Parallelism}{23}{}%
\contentsline {subsubsection}{\numberline {6.2.1}Forward Pass}{23}{}%
\contentsline {subsubsection}{\numberline {6.2.2}Backward Pass}{25}{}%
\contentsline {subsection}{\numberline {6.3}MLP with Tensor Parallelism}{27}{}%
\contentsline {subsubsection}{\numberline {6.3.1}Forward Pass}{27}{}%
\contentsline {subsubsection}{\numberline {6.3.2}Backward Pass}{28}{}%
\contentsline {section}{\numberline {7}Data Parallelism (DP)}{29}{}%
\contentsline {subsection}{\numberline {7.1}DP Overview and Transformer Flow}{29}{}%
\contentsline {subsection}{\numberline {7.2}MHA Backward under DP}{30}{}%
\contentsline {subsection}{\numberline {7.3}MLP Backward under DP}{31}{}%
\contentsline {section}{\numberline {8}Hybrid Data + Tensor Parallelism (DP + TP)}{32}{}%
\contentsline {subsection}{\numberline {8.1}DP+TP Overview and Communication Patterns}{32}{}%
\contentsline {section}{\numberline {9}Summary and Practical Takeaways}{33}{}%
