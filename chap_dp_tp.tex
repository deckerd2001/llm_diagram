% ==========================================================
% 8. Combining Data Parallelism (DP) and Tensor Parallelism (TP)
% ==========================================================
\section{Combining Data Parallelism (DP) and Tensor Parallelism (TP)}
\label{sec:dp_tp}

In previous sections we have treated tensor parallelism (TP, Section~\ref{sec:tp})
and data parallelism (DP, Section~\ref{sec:dp}) separately. In practice, large-scale
training almost always combines both:

\begin{itemize}
  \item \textbf{Tensor parallelism} splits large weight matrices and
        activations across $N_T$ devices inside a \emph{model shard}.
  \item \textbf{Data parallelism} replicates that model shard across
        $N_D$ different mini-batches and synchronizes gradients.
\end{itemize}

The total number of devices is therefore
\[
  N_{\text{devices}} = N_D \times N_T.
\]

From the point of view of the computation graph:

- Inside one TP group, the forward and backward graphs are identical to
  those in Section~\ref{sec:tp} (TP-only).
- Across DP replicas, the wrapping logic is identical to Section~\ref{sec:dp}
  (DP-only): each replica sees a different mini-batch and gradients are
  averaged across replicas.

The main difference from the previous chapters is therefore not in the
shape of the per-layer graphs, but in how devices are organized into
parallel groups and how multiple kinds of collective communication
(All-Reduce and All-Gather) interact.

% ------------------------ 8.1 Parallel Groups -------------------------
\subsection{Parallel Groups and Layout}

We conceptually arrange devices into a 2D grid:

\begin{itemize}
  \item \textbf{TP dimension}: within each row, $N_T$ devices form a
        tensor-parallel group and jointly store a single model shard.
  \item \textbf{DP dimension}: along the column dimension, $N_D$ copies of
        that TP group process different shards of the batch.
\end{itemize}

In other words:

- Each TP group behaves like the TP-only model of Section~\ref{sec:tp}, but only sees
  a local shard of the batch.
- Each DP “column” behaves like the DP-only setup of Section~\ref{sec:dp}, but each
  replica is itself a TP group instead of a single device.

If the global batch has size $B$, we split it across data-parallel groups
as
\[
  B = N_D \cdot B_{\text{local}},
\]
so that DP replica $d$ sees $\mathbf{X}_d \in \mathbb{R}^{B_{\text{local}}
\times S \times D}$ as its input. Within each replica, $\mathbf{X}_d$ is
fed into the TP-sharded model exactly as in Section~\ref{sec:tp}: single-node
matmuls are replaced by column-parallel / row-parallel pairs across the
$N_T$ shards.

Figure~\ref{fig:dp_tp_overall_flow} summarizes this layout and should be
read as a DP+TP version of the single-node overview from
Figure~\ref{fig:single_node_overall} and the TP overview in
Figure~\ref{fig:tp_overall_flow}.

\begin{figure}[htbp]
  \centering
  \input{transformer_overall_flow_DP_TP.tex}
  \caption{Overall Transformer layer with combined data and tensor
  parallelism. Horizontally, each tensor-parallel group of size $N_T$
  jointly implements a sharded version of the layer (as in
  Section~\ref{sec:tp}). Vertically, $N_D$ such groups form a data-parallel grid:
  each row processes a different mini-batch shard, and gradients are
  synchronized across rows. All-Reduce and All-Gather collectives are
  annotated where they appear.}
  \label{fig:dp_tp_overall_flow}
\end{figure}

% ------------------------ 8.2 From Single-Node to TP to DP+TP --------
\subsection{From Single-Node to TP to DP+TP}

It is useful to view DP+TP as a sequence of incremental modifications to
the single-node computation of Section~\ref{sec:sn}:

\paragraph{Single node (Section~\ref{sec:sn}).}
All parameters and activations live on a single device. Each linear layer
is a single matmul, and no collectives are needed.

\paragraph{Tensor parallelism only (Section~\ref{sec:tp}).}
We split large matrices across $N_T$ devices:

\begin{itemize}
  \item \textbf{Column-parallel} linears:
        weights are split along the output dimension. Each shard computes
        a slice of the output; later operations either keep that sharded
        representation or perform an All-Gather to reconstruct the full
        tensor.
  \item \textbf{Row-parallel} linears:
        weights and inputs are split along the input dimension. Each shard
        computes a partial result, and an All-Reduce is used to sum
        partial outputs.
\end{itemize}

Within a single TP group, all devices see the same batch and activations
are sharded along feature dimensions. Collectives (All-Reduce and possibly
All-Gather) appear \emph{inside} the layer graphs.

\paragraph{Data parallelism only (Section~\ref{sec:dp}).}
We keep the single-node graphs intact but replicate them across $N_D$
devices, each with a different mini-batch shard. The only collective is
an All-Reduce over parameter gradients at the end of the backward pass.

\paragraph{Combining DP and TP.}
In DP+TP we apply both modifications at once:

\begin{itemize}
  \item Each data-parallel replica is itself a TP group of $N_T$ devices.
        Inside that group, all TP sharding and collectives from
        Section~\ref{sec:tp} apply as-is.
  \item Across data-parallel replicas, we perform gradient All-Reduces as
        in Section~\ref{sec:dp}, now applied to \emph{sharded} parameters (the TP
        shards of each weight matrix).
\end{itemize}

Thus, the internal structure of each MHA/MLP/output block on a \emph{per
shard} basis is unchanged; the only differences lie in how many devices
share each tensor and which collectives connect them.

% ------------------------ 8.3 Forward Pass under DP+TP ---------------
\subsection{Forward Pass under DP+TP}

From the perspective of a single TP shard inside DP+TP, the forward pass
is the same as in the TP-only model (Section~\ref{sec:tp}):

\begin{itemize}
  \item the input is the local mini-batch shard $\mathbf{X}_d$ of shape
        $[B_{\text{local}}, S, D]$;
  \item column-parallel and row-parallel linears are applied as before;
  \item intermediate activations (e.g., per-head Q/K/V, intermediate MLP
        features) are sharded across the $N_T$ devices.
\end{itemize}

The main new ingredient is the use of \textbf{All-Gather} on activations
in certain places where a subsequent operation requires the full (unsharded)
tensor.

Common examples include:

\begin{itemize}
  \item \textbf{Vocabulary-parallel output projection.}
        If the final LM head weight $\mathbf{W}_{\text{lm}}$ is sharded
        across TP devices along the vocabulary dimension, each shard
        produces logits for a subset of the vocabulary. To compute the
        softmax and loss with the full vocabulary on each device, we
        perform an All-Gather of the partial logits slices to assemble the
        full $[B_{\text{local}}, S, V]$ tensor.
  \item \textbf{Embedding layers.}
        Similarly, if the token embedding matrix is sharded along the
        vocabulary dimension, embedding lookups may produce sharded
        embeddings that must be All-Gathered before being passed into a
        subsequent layer that expects an unsharded $[B_{\text{local}}, S,
        D]$ representation.
  \item \textbf{Non-sharded operations.}
        Any operation that conceptually acts on the full hidden dimension
        (e.g., a global normalization or a non-sharded residual branch)
        may require an All-Gather of the sharded activations before it can
        be applied, unless the model has been carefully arranged so that
        all such operations either run locally on shards or are replaced
        by TP-friendly variants.
\end{itemize}

All-Reduce and All-Gather thus play complementary roles inside a TP group:

\begin{itemize}
  \item \textbf{All-Reduce} combines partial results that were computed on
        a sharded input (row-parallel linears, sharded gradients).
  \item \textbf{All-Gather} reassembles tensor slices that were produced
        by column-parallel linears or vocabulary-parallel projections
        when the next operation needs the full tensor.
\end{itemize}

Data parallelism does not change these intra-group collectives; it only
changes which mini-batch $\mathbf{X}_d$ is fed into the TP group.

% ------------------------ 8.4 Backward Pass and Gradient Sync --------
\subsection{Backward Pass and Gradient Synchronization}

In DP+TP, the backward pass combines:

\begin{itemize}
  \item the TP-only backward graphs from Section~\ref{sec:tp}, including all
        intra-group All-Reduce and All-Gather operations on activations
        and partial gradients, and
  \item the DP-only gradient synchronization from Section~\ref{sec:dp}, now applied
        to the sharded parameters of each TP group.
\end{itemize}

Concretely, for a given parameter tensor $W$:

\begin{itemize}
  \item In TP-only:
    \begin{itemize}
      \item $W$ is split into shards $W^{(t)}$ over $t=0,\dots,N_T-1$,
            each stored on a different device inside a TP group.
      \item Backward through the local matmul yields a local gradient
            $\nabla W^{(t)}$ on each shard.
      \item If $W$ appears in a row-parallel configuration, partial
            gradients w.r.t.\ inputs are All-Reduced across $t$.
    \end{itemize}
  \item In DP+TP:
    \begin{itemize}
      \item The above TP logic is applied independently within each DP
            replica $d$, yielding $\nabla W^{(t,d)}$ on shard $t$ in
            replica $d$.
      \item \emph{Across} replicas, we perform an All-Reduce over $d$ for
            each shard $t$:
            \[
              \nabla W^{(t)}
                = \frac{1}{N_D}
                  \sum_{d=0}^{N_D-1} \nabla W^{(t,d)}.
            \]
            This is a data-parallel gradient synchronization, but now
            replicated over all TP shards in parallel.
    \end{itemize}
\end{itemize}

Thus, every parameter shard participates in two types of collectives:

\begin{itemize}
  \item \textbf{TP collectives} inside a DP replica:
        All-Reduce/All-Gather across $t$ (row-/column-parallel logic).
  \item \textbf{DP collectives} across replicas:
        All-Reduce across $d$ for each shard index $t$.
\end{itemize}

The combination ensures that, at the end of the backward pass, all TP
groups in all DP replicas hold identical, properly aggregated gradients
for their local shards $W^{(t)}$, so that the optimizer update is
consistent across the entire system.

% ------------------------ 8.5 Communication Summary ------------------
\subsection{Communication Summary and Differences from TP-only / DP-only}

We summarize the main differences compared to TP-only (Section~\ref{sec:tp}) and
DP-only (Section~\ref{sec:dp}):

\paragraph{Compared to TP-only.}

\begin{itemize}
  \item \textbf{Same per-layer sharding.}
        Inside one TP group, the forward/backward graphs, the placement of
        All-Reduce and All-Gather, and the tensor shapes are identical to
        the TP-only case.
  \item \textbf{Additional gradient All-Reduce over DP replicas.}
        Every parameter shard now participates in an extra All-Reduce over
        the DP dimension to average gradients across mini-batch shards.
  \item \textbf{No change in local activations.}
        Activation memory and compute patterns inside a TP group do not
        change when DP is added; DP only affects which samples are seen by
        each replica and how parameter gradients are combined.
\end{itemize}

\paragraph{Compared to DP-only.}

\begin{itemize}
  \item \textbf{Sharded model within each replica.}
        Instead of a full copy of the model per DP replica, each replica
        contains a TP-sharded model: parameters and some activations are
        split across $N_T$ devices.
  \item \textbf{More frequent collectives.}
        DP-only uses All-Reduce once per step per parameter tensor. In
        DP+TP, we additionally have intra-replica All-Reduce/All-Gather
        operations inside most layers (MHA, MLP, embeddings, LM head).
  \item \textbf{Per-device memory and compute.}
        TP reduces per-device parameter and activation memory by
        approximately $1/N_T$, at the cost of extra intra-layer
        communication. DP further reduces per-device activation memory by
        a factor of $1/N_D$ by splitting the batch.
\end{itemize}

Overall, DP+TP can be seen as:

\begin{quote}
  “Take the TP-sharded Transformer layer of Section~\ref{sec:tp}, run it on different
  mini-batch shards as in Section~\ref{sec:dp}, and add one more All-Reduce per
  parameter shard to average gradients across replicas.”
\end{quote}

All-Gather operations appear wherever TP has produced sharded activations
that must be globally visible (e.g., logits over the full vocabulary),
while All-Reduce is used both to combine partial TP results and to
aggregate gradients across data-parallel replicas.
