% ==========================================================
% 1. Introduction (Korean)
% ==========================================================
\section{서론}

Transformer와 그 변형 모델들은 현재 대규모 언어 및 비전 모델에서 사실상 표준 아키텍처가 되었다.
최신 시스템들은 수백억에서 수천억 개의 파라미터를 가진 모델을 여러 가속기에 분산시켜 학습하고 서빙하기 위해, 텐서 병렬화(tensor parallelism), 데이터 병렬화(data parallelism), 파이프라인 병렬화(pipeline parallelism) 등 여러 형태의 병렬화를 결합해서 사용한다.

이러한 고수준 아이디어들은 널리 알려져 있지만, 실제 실행 관점의 세부 내용은 다양한 코드베이스, 논문, 블로그 포스트 등에 흩어져 있다.
그 결과, 경험이 많은 실무자조차도 다음과 같은 아주 구체적인 질문에 쉽게 답하기 어려운 경우가 많다.
\begin{itemize}
  \item 단일 Transformer 레이어 안에서, 순전파와 역전파 각각에 어떤 텐서와 행렬 곱(matrix multiplication)이 정확히 등장하는가?
  \item Attention과 MLP를 역전파할 때, 큰 행렬 곱은 실제로 몇 번이나 수행되는가?
  \item 텐서 병렬화나 데이터 병렬화를 도입했을 때, All-Reduce나 All-Gather 연산은 정확히 어디에 등장하는가?
\end{itemize}

이 문서는 위와 같은 질문들에 답하기 위한 \emph{시각적이며 차원 중심적인(visual, dimension-oriented)} 가이드이다.
새로운 아키텍처나 새로운 학습 방법을 제안하지 않으며, 이미 널리 사용되는 기법들—바닐라 Transformer, 역전파, 그리고 일반적인 병렬 학습 스킴—을 다시 조직화하여, 그 구조와 텐서 차원, 비용을 한눈에 파악할 수 있도록 정리하는 것이 목적이다.

\subsection{범위와 목표}

이 문서의 범위는 의도적으로 좁게 잡았다.
\begin{itemize}
  \item 멀티헤드 자기어텐션(MHA), 위치별 피드포워드(MLP/FFN) 블록, 잔차 연결(residual connection), 레이어 정규화(layer normalization)를 포함하는 \emph{표준 Transformer 블록}에만 집중한다.
  \item 전체 end-to-end 학습 파이프라인이 아니라, 입력 임베딩과 출력 프로젝션을 포함한 \emph{단일 레이어 수준}에서의 동작에 초점을 맞춘다.
  \item 다양한 아키텍처 변형이나 실험 결과가 아니라, \emph{레이어 내부에서 텐서와 기울기가 어떻게 흐르는지} 그리고 서로 다른 병렬화 전략이 이 흐름을 어떻게 변화시키는지에 초점을 둔다.
\end{itemize}

이 범위 안에서 우리의 목표는 다음과 같다.
\begin{itemize}
  \item 단일 Transformer 레이어의 순전파와 역전파 계산 그래프를 텐서 차원까지 포함하여 \emph{명시적으로 드러내는 것}.
  \item 텐서 병렬화, 데이터 병렬화, 그리고 두 방법을 결합한 경우에, 이러한 그래프들이 \emph{어떻게 바뀌거나(혹은 거의 바뀌지 않는지)}를 보여주는 것.
  \item 실무자가 계산량, 메모리 사용량, 통신 비용을 가늠하는 데 도움이 되는 \emph{직관적인 멘탈 모델과 간단한 규칙}을 제공하는 것.
\end{itemize}

이 문서에서 다루는 내용은 모두 \emph{표준 역전파와 표준 병렬 학습 기법}을 다시 풀어 쓴 것이다.
기여의 성격은 철저히 설명적(expository)이므로, 새로운 알고리즘을 제안하기보다는 이미 알려진 내용을 통합하고 명료하게 정리하는 데 초점을 둔다.

\subsection{대상 독자}

이 가이드는 다음과 같은 독자를 대상으로 한다.
\begin{itemize}
  \item 기초적인 선형대수 및 기울기, 역전파 개념에 익숙한 독자.
  \item Transformer에 대해 최소한의 상위 수준 이해(예: Q/K/V, attention weight, MLP 블록, 잔차 연결)를 갖춘 독자.
  \item 특히 분산 환경에서, 레이어 내부에서 순전파와 역전파 동안 \emph{실제로 어떤 일이 일어나는지}를 구체적이고 구현 지향적인 관점에서 이해하고자 하는 독자.
\end{itemize}

이 문서의 다이어그램과 표기법은 특정 코드베이스에 종속되지 않으면서도, 대규모 모델 프레임워크들이 실제로 모델을 구현하는 방식과 최대한 가깝게 맞추도록 설계하였다.

\subsection{문서 구성}

이 문서는 간단한 스칼라 기울기에서 출발하여, 완전한 병렬 Transformer 레이어에 이르기까지 단계적으로 설명을 확장해 나간다.

\begin{itemize}
  \item Section~\ref{sec:background}에서는 텐서, 선형 레이어, MLP, 역전파의 기본 아이디어를 다루는 가벼운 신경망 복습을 제공한다. 이는 표기와 직관을 고정하기 위한 것으로, 이미 익숙한 독자는 가볍게 훑어보아도 된다.
  \item Section~\ref{sec:background}에서는 또한, 시퀀스 모델에 왜 자기어텐션이 필요한지, 그리고 표준 Transformer 블록(임베딩, MHA, MLP, 잔차, 정규화)의 상위 구조가 어떻게 구성되는지를 설명한다.
  \item Section~\ref{sec:gradients-basics}에서는 연산자 관점에서 기울기와 역전파를 소개하며, 이후 다이어그램에서 사용할 추상적인 순전파/역전파 노드를 정의한다.
  \item Section~\ref{sec:graph-conventions}에서는 그래프 표기법을 정의한다. 텐서, 파라미터, 기울기, 텐서 차원, 통신 연산(All-Reduce, All-Gather 등)을 그림에서 어떻게 표현하는지를 설명한다.
  \item 이후 섹션들에서는 단일 노드(single-node) Transformer 레이어를 구축한다. 임베딩, MHA, MLP, 출력 프로젝션 각각에 대해 순전파 및 역전파 계산 그래프를 상세히 전개한다.
  \item 후반부 섹션에서는 텐서 병렬화와 데이터 병렬화를 소개하고, 마지막으로 두 방법을 결합한 DP+TP 설정을 다룬다. 여기서 각 데이터 병렬 복제본이 하나의 텐서 병렬 그룹을 이루는 구조를 명시한다.
\end{itemize}

주로 병렬화의 상위 구조에 관심이 있는 독자는 배경 섹션(Section~\ref{sec:background})을 대략적으로 읽어 본 뒤, 병렬화 관련 섹션의 다이어그램에 집중해도 좋다.
반대로 Transformer 레이어 내부의 역전파를 자세히 이해하고 싶은 독자는, 먼저 단일 노드 그래프를 따라가며 기본적인 순전파·역전파 구조를 익힌 뒤, 병렬 버전들을 이 기본 그래프를 구조적으로 변형한 것으로 보는 관점이 도움이 될 것이다.
