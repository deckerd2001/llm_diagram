\renewcommand{\arraystretch}{1.2}
\small

% -------- Operations (Ops) --------
\begin{center}
\textbf{Operations (Ops)}
\begin{tabular}{llll}
\hline
\textbf{Abbrev} & \textbf{Name} & \textbf{Type / Shape} & \textbf{Notes} \\
\hline
S     & Softmax                     & op            & Over vocab axis $V$; outputs probabilities $\mathbf{P}$. \\
CE    & Cross-Entropy               & op            & Usually \emph{sparse} CE consuming label indices $\mathbf{Y}$. \\
ARG   & Argmax (greedy)             & op            & $\operatorname*{argmax}_V$ to get token ids (no gradient). \\
TOP-$k$ & Top-$k$ / sampling        & op            & Optional decoding path; no gradient. \\
T     & Transpose                   & op            & E.g., $\widetilde{\mathbf{W}}_{\text{lm}}^{T} \in \mathbb{R}^{V\times D}$. \\
$\mathrm{BC}_{B,S}(\cdot)$ & Broadcast & op       & Expand $[V] \!\to\! [B,S,V]$ for bias add. \\
dS    & Softmax backward            & op            & With CE: $\mathrm{d}\mathbf{Z}_{\text{bias}}=\mathbf{P}-\text{onehot}(\mathbf{Y})$. \\
dAddB & Addition (Bias) backward    & op            & Sends $\mathrm{d}\mathbf{Z}_{\text{bias}}$ to matmul and $\sum_{B,S}$.\\
$\sum_{B,S}$ & Summation            & op            & Yields $\mathrm{d}\widetilde{\mathbf{b}}_{\text{lm}}$.\\
\hline
\end{tabular}
\end{center}

\vspace{0.8em}

% -------- Data Tensors (Values) --------
\begin{center}
\textbf{Data Tensors (Values)}
\begin{tabular}{lllp{0.46\linewidth}}
\hline
\textbf{Symbol} & \textbf{Name} & \textbf{Shape} & \textbf{Notes} \\
\hline
$\mathbf{A}_{\text{out}}$ & Transformer output (hidden) & $[B,S,D]$ & Final hidden from the Transformer block(s). \\
$\widetilde{\mathbf{W}}_{\text{lm}}$ & LM head weight (tied) & $[D,V]$ & Typically tied to $\mathbf{E}^{T}$. \\
$\widetilde{\mathbf{b}}_{\text{lm}}$ & LM head bias           & $[V]$    & Broadcast-added over $[B,S,V]$. \\
$\mathbf{Z}_{\text{lin}}$ & Logits (linear output) & $[B,S,V]$ & $\mathbf{A}_{\text{out}}\widetilde{\mathbf{W}}_{\text{lm}}$. \\
$\mathbf{Z}_{\text{bias}}$ & Logits (final/Softmax input) & $[B,S,V]$ & $\mathbf{Z}_{\text{lin}}+\widetilde{\mathbf{b}}_{\text{lm}}$. \\
$\mathbf{P}$ & Probabilities                     & $[B,S,V]$ & $\mathrm{softmax}(\mathbf{Z}_{\text{bias}})$. \\
$\mathbf{Y}$ & Target token ids                  & $[B,S]$   & Ground-truth indices (sparse labels). \\
$\mathcal{L}$ & Loss                              & scalar or $[B,S]$ & Typically mean over $B,S$. \\
$\mathrm{d}\mathcal{L}$ & Loss gradient          & scalar-grad & Starting signal for backward pass. \\
$\mathrm{d}\mathbf{Z}_{\text{bias}}$  & Final Logits gradient  & $[B,S,V]$  & From CE+Softmax: $\mathbf{P}-\text{onehot}(\mathbf{Y})$. \\
$\mathrm{d}\mathbf{Z}_{\text{lin}}$ & Linear output grad & $[B,S,V]$ & Same as $\mathrm{d}\mathbf{Z}_{\text{bias}}$. \\
$\mathrm{d}\widetilde{\mathbf{W}}_{\text{lm}}$ & LM weight grad & $[D,V]$ & $=\mathbf{A}_{\text{out}}^{T}\mathrm{d}\mathbf{Z}_{\text{lin}}$. \\
$\mathrm{d}\widetilde{\mathbf{b}}_{\text{lm}}$ & LM bias grad & $[V]$ & $=\sum_{B,S}\mathrm{d}\mathbf{Z}_{\text{bias}}$. \\
$\mathrm{d}\mathbf{A}_{\text{out}}$ & Hidden grad & $[B,S,D]$  & $=\mathrm{d}\mathbf{Z}_{\text{lin}}\,\widetilde{\mathbf{W}}_{\text{lm}}^{T}$. \\
\hline
\multicolumn{4}{l}{\textbf{Shapes:}\ $B$=batch,\ $S$=sequence length,\ $D$=hidden dim,\ $V$=vocab size.} \\
\hline
\end{tabular}
\end{center}
