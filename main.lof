\contentsline {figure}{\numberline {1}{\ignorespaces Single-node Transformer layer: overall forward and backward flow. Solid arrows denote forward activations, and dashed arrows denote gradients flowing backward from the loss $\mathcal {L}$ through the output projection, MLP, MHA, and back to the input embeddings.}}{22}{}%
\contentsline {figure}{\numberline {2}{\ignorespaces Input embedding forward pass. Token IDs are mapped to token embeddings using $\mathbf {E}$, positional embeddings $\mathbf {P}$ are added, and optional dropout yields the initial hidden states $\mathbf {X} \in [B,S,D]$.}}{23}{}%
\contentsline {figure}{\numberline {3}{\ignorespaces Multi-head attention forward pass on a single node. Input activations $\mathbf {X}$ are layer-normalized, projected to $\mathbf {Q}$, $\mathbf {K}$, and $\mathbf {V}$, processed by scaled dot-product attention over the sequence, concatenated across heads, and projected back to $[B,S,D]$ with an output projection and residual connection.}}{25}{}%
\contentsline {figure}{\numberline {4}{\ignorespaces Multi-head attention backward pass. The diagram shows how gradients from $\mathrm {d}\mathbf {A}_{\text {out}}$ are propagated through the output projection, attention heads, QKV projection matrices, and layer normalization to yield $\mathrm {d}\mathbf {X}$ and all associated weight and bias gradients.}}{27}{}%
\contentsline {figure}{\numberline {5}{\ignorespaces Feed-forward (MLP) forward pass. Hidden states are layer-normalized, projected up to dimension $D_{\text {ff}}$, passed through a non-linearity and dropout, projected back to $D$, and combined with the input via a residual connection.}}{28}{}%
\contentsline {figure}{\numberline {6}{\ignorespaces Feed-forward (MLP) backward pass. The figure shows how $\mathrm {d}\mathbf {Y}$ splits through the residual and FFN path, and how gradients flow through the down-projection, activation, up-projection, and layer normalization to yield $\mathrm {d}\mathbf {X}'$ and the corresponding parameter gradients.}}{29}{}%
\contentsline {figure}{\numberline {7}{\ignorespaces Output projection and loss: the final hidden states $\mathbf {A}_{\text {out}}$ are mapped to logits by the language model head $(\mathbf {W}_{\text {lm}}, \mathbf {b}_{\text {lm}})$, converted to probabilities by softmax, and compared with target tokens using cross-entropy.}}{29}{}%
\contentsline {figure}{\numberline {8}{\ignorespaces Output projection backward pass. Gradients from the loss are propagated through cross-entropy and softmax to the logits, then through the language model projection to produce $\mathrm {d}\mathbf {A}_{\text {out}}$ and parameter gradients $\mathrm {d}\mathbf {W}_{\text {lm}}$ and $\mathrm {d}\mathbf {b}_{\text {lm}}$.}}{30}{}%
\contentsline {figure}{\numberline {9}{\ignorespaces Overall Transformer layer with tensor parallelism. Each large linear layer from the single-node model is replaced by $N_T$ smaller matmuls on different devices. Colored arrows indicate where collective communication (e.g.\ All-Reduce, All-Gather) is required to assemble partial results, while local computations remain identical to those in Figure~\ref {fig:single_node_overall}.}}{32}{}%
\contentsline {figure}{\numberline {10}{\ignorespaces Multi-head attention forward pass with tensor parallelism. Q/K/V projections are implemented as column-parallel linears so that each device owns a subset of the heads. Attention for each local head is computed independently, and the resulting head outputs are combined using a row-parallel output projection followed by an All-Reduce to recover the same $\mathbf {A}_{\text {out}}$ as in the single-node computation.}}{34}{}%
\contentsline {figure}{\numberline {11}{\ignorespaces Multi-head attention backward pass with tensor parallelism. Each device backpropagates through its local Q/K/V projections and attention heads. Gradients with respect to the shared normalized input are summed across devices via All-Reduce, and parameter gradients are accumulated locally for each shard $W_Q^{(t)}, W_K^{(t)}, W_V^{(t)}$ and $W_O^{(t)}$.}}{36}{}%
\contentsline {figure}{\numberline {12}{\ignorespaces MLP forward pass with tensor parallelism. The up-projection is implemented as a column-parallel linear so that each device holds a subset of the intermediate features. The down-projection is row-parallel, and an All-Reduce over devices reconstructs the full $\mathbf {Z}_{\text {down}}$, after which dropout and the residual connection are applied locally.}}{38}{}%
\contentsline {figure}{\numberline {13}{\ignorespaces MLP backward pass with tensor parallelism. Each device backpropagates through its local up- and down-projection shards. The only collective in the backward path is an All-Reduce that sums the partial input gradients $\mathrm {d}\mathbf {H}^{(t)}$ across devices to obtain the full $\mathrm {d}\mathbf {H}$. Parameter gradients remain local to each shard.}}{39}{}%
\contentsline {figure}{\numberline {14}{\ignorespaces Overall Transformer layer under data parallelism. Each device holds a full copy of the model and processes a different shard of the input batch ($\mathbf {X}_i$, $\mathbf {X}_j$, \dots ). Forward and backward passes are performed locally as in the single-node case, and gradients for each block (MHA, MLP, output projection) are synchronized across replicas via All-Reduce.}}{41}{}%
\contentsline {figure}{\numberline {15}{\ignorespaces Multi-head attention backward pass under data parallelism. Each replica computes local gradients w.r.t.\ $W_Q$, $W_K$, $W_V$, and $W_O$ using its own mini-batch. Red dashed arrows indicate All-Reduce operations that aggregate these local gradients across all data-parallel replicas to form the global gradients used by the optimizer. The internal structure of the backward graph matches the single-node case.}}{43}{}%
\contentsline {figure}{\numberline {16}{\ignorespaces MLP backward pass under data parallelism. Each replica computes local gradients for the up- and down-projection weights and biases using its own mini-batch. Red boxes and dashed arrows indicate All-Reduce operations that aggregate these local parameter gradients across replicas. The structure of the backward computation inside each replica is identical to the single-node graph.}}{45}{}%
\contentsline {figure}{\numberline {17}{\ignorespaces Overall Transformer layer with combined data and tensor parallelism. Horizontally, each tensor-parallel group of size $N_T$ jointly implements a sharded version of the layer (as in Section~\ref {sec:tp}). Vertically, $N_D$ such groups form a data-parallel grid: each row processes a different mini-batch shard, and gradients are synchronized across rows. All-Reduce and All-Gather collectives are annotated where they appear.}}{47}{}%
