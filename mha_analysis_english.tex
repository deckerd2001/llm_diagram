% ===================== MHA Computational Complexity Analysis =====================
\documentclass[10pt]{article}

% --- Page setup ---
\usepackage[margin=1in]{geometry}
\usepackage{setspace}

% --- Encoding & fonts ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% --- Math / graphics / tikz / tables ---
\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows, positioning, calc}
\usepackage{xcolor}

% --- Hyperlinks ---
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue}

% ===== Inline Icon Styles (match diagram styles) =====
\tikzset{
  icon-mul/.style={draw, circle, fill=white, inner sep=0.8pt},
  icon-add/.style={draw, circle, fill=white, inner sep=0.6pt},
  icon-aux/.style={draw, rectangle, fill=white, inner sep=1.2pt},
  icon-sum/.style={draw, circle, fill=white, inner sep=0.3pt}
}

% ===== Inline Icon Commands =====
\newcommand{\iconMatMul}{%
  \tikz[baseline=-0.6ex]\node[icon-mul]{$\scriptsize\bullet$};%
}
\newcommand{\iconAdd}{%
  \tikz[baseline=-0.6ex]\node[icon-add]{\scriptsize$+$};%
}
\newcommand{\iconAux}[1]{%
  \tikz[baseline=-0.6ex]\node[icon-aux]{\scriptsize #1};%
}
\newcommand{\iconSum}{%
  \tikz[baseline=-0.6ex]\node[icon-sum]{\tiny$\sum$};%
}

% --- Title & Authors ---
\title{Computational Flow and Complexity Analysis\\of Multi-Head Attention Mechanism\\\large Understanding Operations, Dimensions, and Computational Costs}
\author{Technical Report}
\date{\today}

% --- Theorem-like environments ---
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

\begin{document}
\maketitle

\begin{abstract}
This paper provides a systematic analysis of the Multi-Head Attention (MHA) mechanism, the core component of Transformer-based large language models (LLMs), from a computational graph perspective. We begin by mathematically defining and characterizing fundamental operations—matrix multiplication, element-wise operations, softmax, layer normalization, transpose, and reshape—that are essential for understanding MHA diagrams. We then trace the complete data flow of both forward and backward passes step-by-step, quantitatively deriving the time complexity (FLOPs) and space complexity (memory) at each stage. Special attention is given to analyzing how the $O(S^2)$ complexity of self-attention affects long sequence processing, and we discuss practical optimization strategies including FlashAttention, Multi-Query Attention, and gradient checkpointing.
\end{abstract}

\paragraph{Keywords} Transformer; Multi-Head Attention; Computational Complexity; Memory Analysis; Forward Pass; Backward Pass; Gradient Computation; LLM Optimization.

\tableofcontents
\newpage

% ===================== 1. Introduction =====================
\section{Introduction}
\label{sec:intro}

The Transformer architecture~\cite{vaswani2017attention} has become the foundation of modern natural language processing and large language models (LLMs). State-of-the-art models such as the GPT series, BERT, and LLaMA all utilize the Multi-Head Attention (MHA) mechanism as their core component, and the performance and efficiency of these models heavily depend on the effective implementation of MHA.

\subsection{Motivation}

As the scale of LLMs increases dramatically (e.g., GPT-3 with 175B parameters, GPT-4 with an estimated 1.7T parameters), accurate understanding of computational and memory costs for each operation has become essential. Key considerations include:

\begin{itemize}
  \item \textbf{Increasing sequence length $S$}: As context windows expand from 4K $\rightarrow$ 32K $\rightarrow$ 128K, self-attention with $O(S^2)$ complexity becomes the primary bottleneck
  \item \textbf{Growing model dimension $D$}: Larger embedding dimensions increase the computational cost of linear projections
  \item \textbf{Batch processing}: Optimizing batch size $B$ to maximize GPU utilization
  \item \textbf{Gradient computation}: Memory requirements for the backward pass are 2-3x higher than the forward pass
\end{itemize}

\subsection{Contributions}

The contributions of this paper are as follows:

\begin{enumerate}
  \item \textbf{Computational graph-based analysis}: Visualizing MHA data flow through graphs with nodes (operations) and edges (tensors)
  \item \textbf{Detailed operation analysis}: Mathematical definitions and complexity derivations for each primitive operation (MatMul, Add, Softmax, LayerNorm, etc.)
  \item \textbf{Forward \& Backward pass analysis}: Step-by-step FLOPs and memory analysis for forward and backward propagation
  \item \textbf{Scaling laws}: Characterizing complexity scaling with respect to $B$, $S$, $D$, and $N_H$
  \item \textbf{Optimization strategies}: Discussion of practical optimization techniques including FlashAttention and KV caching
\end{enumerate}

\subsection{Organization}

The paper is organized as follows:
\begin{itemize}
  \item \textbf{Section~\ref{sec:graph}}: Computational graph notation and visual conventions
  \item \textbf{Section~\ref{sec:ops}}: Mathematical definitions and characteristics of basic operations
  \item \textbf{Section~\ref{sec:forward}}: Step-by-step complexity analysis of the forward pass
  \item \textbf{Section~\ref{sec:backward}}: Gradient computation analysis of the backward pass
  \item \textbf{Section~\ref{sec:summary}}: Overall complexity summary and optimization strategies
\end{itemize}

% ===================== 2. Computational Graph Notation =====================
\section{Computational Graph Notation}
\label{sec:graph}

\subsection{Graph Structure: Nodes and Edges}

Neural network computation can be represented as a Directed Acyclic Graph (DAG), where:
\begin{itemize}
  \item \textbf{Nodes}: Represent operations
  \item \textbf{Edges}: Represent tensor data flow
  \item \textbf{Edge labels}: Indicate tensor names and shapes (e.g., $\mathbf{X}$ $[B,S,D]$)
\end{itemize}

\subsection{Node Types}

The following node types are used in our diagrams:

\paragraph{Matrix Multiplication} \iconMatMul
\begin{itemize}
  \item \textbf{Symbol}: Circular node with $\bullet$ symbol
  \item \textbf{Meaning}: Matrix multiplication of two tensors
  \item \textbf{Inputs}: Two tensors (one marked with double arrow)
  \item \textbf{Output}: Product tensor
\end{itemize}

\paragraph{Element-wise Addition} \iconAdd
\begin{itemize}
  \item \textbf{Symbol}: Circular node with $+$ symbol
  \item \textbf{Meaning}: Element-wise addition of two tensors (broadcasting allowed)
  \item \textbf{Usage}: Residual connections, bias addition
\end{itemize}

\paragraph{Auxiliary Operations} \iconAux{LN}, \iconAux{SM}, \iconAux{S}, \iconAux{T}, \iconAux{R}, \iconAux{C}, \iconAux{DO}
\begin{itemize}
  \item \textbf{Symbol}: Rectangular node with operation abbreviation
  \item \textbf{Types}:
  \begin{itemize}
    \item \iconAux{LN}: Layer Normalization
    \item \iconAux{SM}: Scale + Mask (for attention scores)
    \item \iconAux{S}: Softmax
    \item \iconAux{T}: Transpose
    \item \iconAux{R}: Reshape (head split/merge)
    \item \iconAux{C}: Concatenate
    \item \iconAux{DO}: Dropout
  \end{itemize}
\end{itemize}

\paragraph{Reduction Operations} \iconSum
\begin{itemize}
  \item \textbf{Symbol}: Small circle with $\sum$ symbol
  \item \textbf{Meaning}: Summation over specific axes (e.g., for bias gradient computation)
  \item \textbf{Label}: Indicates reduction dimensions (e.g., $\sum_{B,S}$)
\end{itemize}

\subsection{Edge Conventions}

\paragraph{Single Arrow $\rightarrow$}
Represents general data flow. In forward pass, carries activations; in backward pass, carries gradients.

\paragraph{Double Arrow $\Rightarrow$}
Explicitly marks the second operand (typically weight matrix) of matrix multiplication \iconMatMul. This clarifies that $W$ is the right operand in $Z = XW$.

\textbf{Example}:
\begin{itemize}
  \item Forward: $\mathbf{X}$ $\rightarrow$ \iconMatMul $\Leftarrow$ $\mathbf{W}$ $\rightarrow$ $\mathbf{Z}$
  \item Meaning: $\mathbf{Z} = \mathbf{X} \mathbf{W}$
\end{itemize}

\subsection{Shape Notation}

All tensors are annotated with their shapes:
\begin{itemize}
  \item $B$: Batch size
  \item $S$: Sequence length
  \item $D$: Model dimension
  \item $N_H$: Number of attention heads
  \item $D_h$: Head dimension ($D_h = D / N_H$)
  \item $V$: Vocabulary size
\end{itemize}

\textbf{Broadcasting notation}: $\mathrm{BC}_{B,S}(\tilde{\mathbf{b}})$ indicates that a bias of shape $[D]$ is broadcast to shape $[B,S,D]$.

% ===================== 3. Understanding Basic Operations =====================
\section{Understanding Basic Operations}
\label{sec:ops}

\subsection{Matrix Multiplication}
\label{subsec:matmul}

\subsubsection{Mathematical Definition}

Matrix multiplication of $\mathbf{A} \in \mathbb{R}^{m \times k}$ and $\mathbf{B} \in \mathbb{R}^{k \times n}$:
\begin{equation}
\mathbf{C} = \mathbf{A}\mathbf{B}, \quad C_{ij} = \sum_{p=1}^{k} A_{ip} B_{pj}
\end{equation}

\subsubsection{Batched Matrix Multiplication}

With additional batch dimension: $\mathbf{A} \in \mathbb{R}^{[B,m,k]}$, $\mathbf{B} \in \mathbb{R}^{[B,k,n]}$
\begin{equation}
\mathbf{C}[b,:,:] = \mathbf{A}[b,:,:] \mathbf{B}[b,:,:] \quad \text{for each } b \in [1, B]
\end{equation}

\subsubsection{Computational Cost (FLOPs)}

Single matrix multiplication $[m,k] \times [k,n] \rightarrow [m,n]$:
\begin{itemize}
  \item Per output element: $k$ multiplications + $(k-1)$ additions $\approx 2k$ FLOPs
  \item Total output elements: $m \times n$
  \item \textbf{Total FLOPs}: $2mnk$
\end{itemize}

Batched matrix multiplication $[B,m,k] \times [B,k,n]$:
\begin{equation}
\text{FLOPs} = B \cdot 2mnk = 2Bmnk
\end{equation}

\subsubsection{Memory Requirements}

\begin{itemize}
  \item Inputs: $Bmk + Bkn$ elements
  \item Output: $Bmn$ elements
  \item \textbf{Total}: $B(mk + kn + mn)$ (multiply by 4 for float32 bytes)
\end{itemize}

\subsubsection{Backward Pass}

Forward: $\mathbf{Z} = \mathbf{X}\mathbf{W}$ where $\mathbf{X} \in \mathbb{R}^{[B,S,D_{\text{in}}]}$, $\mathbf{W} \in \mathbb{R}^{[D_{\text{in}},D_{\text{out}}]}$

Gradients with respect to inputs:
\begin{align}
\frac{\partial L}{\partial \mathbf{X}} &= \frac{\partial L}{\partial \mathbf{Z}} \mathbf{W}^{\top} \quad [B,S,D_{\text{out}}] \times [D_{\text{out}},D_{\text{in}}] \\
\frac{\partial L}{\partial \mathbf{W}} &= \mathbf{X}^{\top} \frac{\partial L}{\partial \mathbf{Z}} \quad [D_{\text{in}},B \cdot S] \times [B \cdot S,D_{\text{out}}]
\end{align}

Each gradient computation also involves matrix multiplication with similar FLOPs.

\subsection{Element-wise Addition}
\label{subsec:add}

\subsubsection{Mathematical Definition}

For two tensors $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{[d_1, d_2, \ldots, d_n]}$ with the same shape:
\begin{equation}
\mathbf{C} = \mathbf{A} + \mathbf{B}, \quad C_{i_1, i_2, \ldots, i_n} = A_{i_1, i_2, \ldots, i_n} + B_{i_1, i_2, \ldots, i_n}
\end{equation}

\subsubsection{Broadcasting}

NumPy/PyTorch broadcasting rules:
\begin{itemize}
  \item Shape $[B,S,D]$ + shape $[D]$ $\rightarrow$ bias broadcast over $(B,S)$
  \item Shape $[B,S,D]$ + shape $[B,S,D]$ $\rightarrow$ element-wise addition
\end{itemize}

\textbf{Example}: Bias addition
\begin{equation}
\mathbf{Y} = \mathbf{X} + \mathrm{BC}_{B,S}(\tilde{\mathbf{b}}), \quad \mathbf{Y}[b,s,:] = \mathbf{X}[b,s,:] + \tilde{\mathbf{b}}
\end{equation}

\subsubsection{Computational Cost}

Element-wise addition:
\begin{equation}
\text{FLOPs} = \text{total number of elements} = \prod_i d_i
\end{equation}

Example: Addition of $[B,S,D]$ tensors requires $BSD$ FLOPs

\subsubsection{Memory}

\begin{itemize}
  \item Inputs: $2 \times \text{size}$ (smaller tensor reused with broadcasting)
  \item Output: $\text{size}$
\end{itemize}

\subsubsection{Backward Pass}

Forward: $\mathbf{C} = \mathbf{A} + \mathbf{B}$

Gradients:
\begin{align}
\frac{\partial L}{\partial \mathbf{A}} &= \frac{\partial L}{\partial \mathbf{C}} \\
\frac{\partial L}{\partial \mathbf{B}} &= \frac{\partial L}{\partial \mathbf{C}}
\end{align}

With broadcasting, sum gradient over broadcast dimensions:
\begin{equation}
\frac{\partial L}{\partial \tilde{\mathbf{b}}} = \sum_{b=1}^{B} \sum_{s=1}^{S} \frac{\partial L}{\partial \mathbf{C}}[b,s,:] \quad \text{(sum over } B, S\text{)}
\end{equation}

This is represented by a \iconSum node in the diagram.

\subsection{Softmax}
\label{subsec:softmax}

\subsubsection{Mathematical Definition}

For vector $\mathbf{x} = [x_1, x_2, \ldots, x_n]$:
\begin{equation}
\text{softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\end{equation}

For multi-dimensional tensors, applied along a specific axis (typically the last axis).

\textbf{Usage in MHA}: For attention scores $\mathbf{A} \in \mathbb{R}^{[B,N_H,S,S]}$, apply softmax along the last axis (key dimension):
\begin{equation}
\mathbf{AS}[b,h,i,:] = \text{softmax}(\mathbf{A}[b,h,i,:])
\end{equation}

\subsubsection{Numerical Stability}

In practice, use the log-sum-exp trick to prevent overflow:
\begin{equation}
\text{softmax}(\mathbf{x})_i = \frac{e^{x_i - \max(\mathbf{x})}}{\sum_{j} e^{x_j - \max(\mathbf{x})}}
\end{equation}

\subsubsection{Computational Cost}

Softmax over dimension of size $n$:
\begin{itemize}
  \item Max computation: $n$ comparisons
  \item Exp computation: $n$ exponentials
  \item Sum computation: $n$ additions
  \item Division: $n$ divisions
  \item \textbf{Total}: $\approx 4n$ operations per vector
\end{itemize}

For tensor $[B,N_H,S,S]$ with softmax along last axis:
\begin{equation}
\text{FLOPs} = B \cdot N_H \cdot S \cdot (4S) = 4BN_H S^2
\end{equation}

\subsubsection{Memory}

\begin{itemize}
  \item Input: $BN_H S^2$
  \item Output: $BN_H S^2$
  \item Intermediate values (max, sum): $BN_H S$
\end{itemize}

\subsubsection{Backward Pass}

Forward: $\mathbf{y} = \text{softmax}(\mathbf{x})$

Gradient:
\begin{equation}
\frac{\partial L}{\partial x_i} = y_i \left( \frac{\partial L}{\partial y_i} - \sum_{j} y_j \frac{\partial L}{\partial y_j} \right)
\end{equation}

Vector form:
\begin{equation}
\frac{\partial L}{\partial \mathbf{x}} = \mathbf{y} \odot \left( \frac{\partial L}{\partial \mathbf{y}} - \langle \mathbf{y}, \frac{\partial L}{\partial \mathbf{y}} \rangle \mathbf{1} \right)
\end{equation}

where $\odot$ is element-wise multiplication and $\langle \cdot, \cdot \rangle$ is inner product.

\subsection{Layer Normalization}
\label{subsec:layernorm}

\subsubsection{Mathematical Definition}

Normalization over feature dimension ($D$) for each sample and token:

Input: $\mathbf{x} \in \mathbb{R}^{D}$ (single token)

\begin{align}
\mu &= \frac{1}{D} \sum_{i=1}^{D} x_i \\
\sigma^2 &= \frac{1}{D} \sum_{i=1}^{D} (x_i - \mu)^2 \\
\hat{x}_i &= \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \\
y_i &= \gamma_i \hat{x}_i + \beta_i
\end{align}

where $\gamma, \beta \in \mathbb{R}^{D}$ are learnable parameters and $\epsilon$ is a small constant for numerical stability (e.g., $10^{-5}$).

\subsubsection{Batch Processing}

For tensor $\mathbf{X} \in \mathbb{R}^{[B,S,D]}$, apply LayerNorm independently at each $(b,s)$ position:
\begin{equation}
\mathbf{Y}[b,s,:] = \text{LayerNorm}(\mathbf{X}[b,s,:])
\end{equation}

\subsubsection{Computational Cost}

Single vector $\mathbf{x} \in \mathbb{R}^{D}$:
\begin{itemize}
  \item Mean computation: $D$ additions
  \item Variance computation: $D$ subtractions, $D$ squares, $D$ additions
  \item Normalization: $D$ subtractions, $D$ divisions, $D$ square roots (amortized)
  \item Scale/shift: $D$ multiplications, $D$ additions
  \item \textbf{Total}: $\approx 6D$ operations
\end{itemize}

Tensor $[B,S,D]$:
\begin{equation}
\text{FLOPs} = B \cdot S \cdot 6D = 6BSD
\end{equation}

\subsubsection{Memory}

\begin{itemize}
  \item Input: $BSD$
  \item Output: $BSD$
  \item Parameters: $2D$ ($\gamma, \beta$)
  \item Cache for backward: $2BS$ ($\mu, \sigma^2$) + $BSD$ ($\hat{\mathbf{X}}$)
\end{itemize}

\subsubsection{Backward Pass}

Values saved from forward: $\mu, \sigma^2, \hat{\mathbf{x}}$

Gradient w.r.t. parameters:
\begin{align}
\frac{\partial L}{\partial \beta} &= \sum_{b,s} \frac{\partial L}{\partial \mathbf{Y}}[b,s,:] \\
\frac{\partial L}{\partial \gamma} &= \sum_{b,s} \frac{\partial L}{\partial \mathbf{Y}}[b,s,:] \odot \hat{\mathbf{X}}[b,s,:]
\end{align}

Gradient w.r.t. input (per token):
\begin{equation}
\frac{\partial L}{\partial \mathbf{x}} = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} \left[ \frac{\partial L}{\partial \mathbf{y}} - \frac{1}{D}\sum \frac{\partial L}{\partial \mathbf{y}} - \hat{\mathbf{x}} \frac{1}{D}\sum \left(\frac{\partial L}{\partial \mathbf{y}} \odot \hat{\mathbf{x}}\right) \right]
\end{equation}

\subsection{Transpose}
\label{subsec:transpose}

\subsubsection{Mathematical Definition}

Matrix transpose $\mathbf{A} \in \mathbb{R}^{[m,n]}$:
\begin{equation}
\mathbf{A}^{\top}_{ij} = \mathbf{A}_{ji}
\end{equation}

For higher-dimensional tensors, exchange (permute) specific axes:
\begin{equation}
\mathbf{B} = \text{permute}(\mathbf{A}, \text{dims}=(0,2,1,3))
\end{equation}

\textbf{Usage in MHA}: Transform $\mathbf{K} \in \mathbb{R}^{[B,N_H,S,D_h]}$ to $\mathbf{K}^{\top} \in \mathbb{R}^{[B,N_H,D_h,S]}$.

\subsubsection{Computational Cost}

Transpose only rearranges data:
\begin{equation}
\text{FLOPs} = 0 \quad \text{(no arithmetic operations)}
\end{equation}

However, changes in memory access patterns can affect cache efficiency.

\subsubsection{Memory}

\begin{itemize}
  \item If not in-place: requires memory for both input and output
  \item Contiguous memory requirement: may need reordering for subsequent operation efficiency
\end{itemize}

\subsubsection{Backward Pass}

Forward: $\mathbf{B} = \text{permute}(\mathbf{A}, \text{dims})$

Backward: Permute gradient in reverse order
\begin{equation}
\frac{\partial L}{\partial \mathbf{A}} = \text{permute}\left(\frac{\partial L}{\partial \mathbf{B}}, \text{inverse\_dims}\right)
\end{equation}

\subsection{Reshape}
\label{subsec:reshape}

\subsubsection{Mathematical Definition}

Change tensor shape while preserving element order:
\begin{equation}
\mathbf{B} = \text{reshape}(\mathbf{A}, \text{new\_shape})
\end{equation}

\textbf{Usage in MHA}:
\begin{itemize}
  \item Head split: $[B,S,D] \rightarrow [B,N_H,S,D_h]$ where $D = N_H \times D_h$
  \item Head merge: $[B,N_H,S,D_h] \rightarrow [B,S,D]$
\end{itemize}

\subsubsection{Computational Cost}

Reshape only changes metadata (shape information):
\begin{equation}
\text{FLOPs} = 0 \quad \text{(no arithmetic operations)}
\end{equation}

\subsubsection{Memory}

\begin{itemize}
  \item Typically a view operation: no additional memory required
  \item May trigger copy if contiguous memory is required
\end{itemize}

\subsubsection{Backward Pass}

Forward: $\mathbf{B} = \text{reshape}(\mathbf{A}, \text{shape}_B)$

Backward: Reshape gradient back to original shape
\begin{equation}
\frac{\partial L}{\partial \mathbf{A}} = \text{reshape}\left(\frac{\partial L}{\partial \mathbf{B}}, \text{shape}_A\right)
\end{equation}

\subsection{Concatenate}
\label{subsec:concat}

\subsubsection{Mathematical Definition}

Combine multiple tensors along a specific axis:
\begin{equation}
\mathbf{C} = \text{concat}([\mathbf{A}_1, \mathbf{A}_2, \ldots, \mathbf{A}_k], \text{dim}=d)
\end{equation}

\textbf{Usage in MHA}: Combine outputs from each head
\begin{equation}
[B,S,N_H,D_h] \xrightarrow{\text{reshape}} [B,S,N_H \times D_h] = [B,S,D]
\end{equation}

(Actually implemented as reshape, but conceptually concatenation)

\subsubsection{Computational Cost}

Concatenate only performs memory copying:
\begin{equation}
\text{FLOPs} = 0 \quad \text{(no arithmetic operations)}
\end{equation}

\subsubsection{Memory}

\begin{itemize}
  \item Inputs: $\sum_i \text{size}(\mathbf{A}_i)$
  \item Output: $\text{size}(\mathbf{C})$ = $\sum_i \text{size}(\mathbf{A}_i)$
\end{itemize}

\subsubsection{Backward Pass}

Forward: $\mathbf{C} = \text{concat}([\mathbf{A}_1, \ldots, \mathbf{A}_k], \text{dim}=d)$

Backward: Split gradient back to original tensors
\begin{equation}
\frac{\partial L}{\partial \mathbf{A}_i} = \text{split}\left(\frac{\partial L}{\partial \mathbf{C}}, \text{dim}=d\right)_i
\end{equation}

\subsection{Dropout}
\label{subsec:dropout}

\subsubsection{Mathematical Definition}

During training, randomly set activations to zero:
\begin{equation}
\mathbf{Y} = \frac{\mathbf{M} \odot \mathbf{X}}{1-p}
\end{equation}

where $\mathbf{M} \sim \text{Bernoulli}(1-p)$ is a binary mask and $p$ is the dropout rate.

During inference: identity operation ($\mathbf{Y} = \mathbf{X}$)

\subsubsection{Computational Cost}

Training:
\begin{itemize}
  \item Mask generation: $\text{size}(\mathbf{X})$ random samples
  \item Element-wise multiplication: $\text{size}(\mathbf{X})$
  \item Scaling: $\text{size}(\mathbf{X})$ divisions
  \item \textbf{Total}: $\approx 2 \times \text{size}(\mathbf{X})$ operations
\end{itemize}

Inference: 0 FLOPs (identity)

\subsubsection{Memory}

\begin{itemize}
  \item Mask: $\text{size}(\mathbf{X})$ bits (can be optimized)
  \item Cache for backward: mask must be stored
\end{itemize}

\subsubsection{Backward Pass}

Forward (training): $\mathbf{Y} = \mathbf{M} \odot \mathbf{X} / (1-p)$

Backward:
\begin{equation}
\frac{\partial L}{\partial \mathbf{X}} = \frac{\mathbf{M} \odot \frac{\partial L}{\partial \mathbf{Y}}}{1-p}
\end{equation}

Gradient is zero at positions where mask is zero.

\newpage

% ===================== 4. Forward Pass Analysis =====================
\section{Forward Pass Analysis}
\label{sec:forward}

Having understood the basic operations, we now analyze the forward pass of Multi-Head Attention step-by-step. For each step, we specify input/output shapes, computational cost (FLOPs), and memory usage.

\subsection{Notation}

\begin{itemize}
  \item $\mathbf{X} \in \mathbb{R}^{[B,S,D]}$: Input hidden states
  \item $\widetilde{\mathbf{W}}_Q, \widetilde{\mathbf{W}}_K, \widetilde{\mathbf{W}}_V \in \mathbb{R}^{[D,D]}$: Query, Key, Value projection weights
  \item $\widetilde{\mathbf{W}}_O \in \mathbb{R}^{[D,D]}$: Output projection weight
  \item $\tilde{\mathbf{b}}_O \in \mathbb{R}^{D}$: Output bias
  \item $N_H$: Number of attention heads
  \item $D_h = D / N_H$: Dimension per head
\end{itemize}

\subsection{Step 0: Layer Normalization}

\paragraph{Operation:} \iconAux{LN}
\paragraph{Input:} $\mathbf{X}$ $[B,S,D]$
\paragraph{Output:} $\mathbf{X}_{\text{norm}}$ $[B,S,D]$

\paragraph{Computation:}
\begin{equation}
\mathbf{X}_{\text{norm}}[b,s,:] = \text{LayerNorm}(\mathbf{X}[b,s,:])
\end{equation}

\paragraph{FLOPs:} $6BSD = O(BSD)$

\paragraph{Memory:}
\begin{itemize}\item Activations: $2BSD$; \item Cache: $2BS + BSD$; \item Parameters: $2D$\end{itemize}

\subsection{Step 1: Q, K, V Projections}

\paragraph{Operation:} \iconMatMul $\times 3$
\paragraph{Input:} $\mathbf{X}_{\text{norm}}$ $[B,S,D]$
\paragraph{Output:} $\mathbf{Q}_{\text{flat}}, \mathbf{K}_{\text{flat}}, \mathbf{V}_{\text{flat}}$ each $[B,S,D]$

\paragraph{FLOPs:} $3 \times 2BSD^2 = 6BSD^2 = O(BSD^2)$
\paragraph{Memory:} Weights: $3D^2$; Outputs: $3BSD$

\subsection{Step 2: Reshape to Multi-Head}

\paragraph{Operation:} \iconAux{R} $\times 3$
\paragraph{Input/Output:} $[B,S,D] \rightarrow [B,N_H,S,D_h]$
\paragraph{FLOPs:} $0$ (metadata operation)

\subsection{Step 3: Attention Scores (Q $\cdot$ K$^{\top}$)}

\paragraph{Operation:} \iconAux{T} + \iconMatMul
\paragraph{Input:} $\mathbf{Q}, \mathbf{K}$ each $[B,N_H,S,D_h]$
\paragraph{Output:} $\mathbf{A}$ $[B,N_H,S,S]$

\paragraph{FLOPs:} $2BS^2D = O(BS^2D)$ \textbf{(Primary $O(S^2)$ bottleneck)}
\paragraph{Memory:} $BN_H S^2$ \textbf{(Memory bottleneck for long sequences)}

\subsection{Step 4: Scale and Mask}

\paragraph{Operation:} \iconAux{SM}
\paragraph{FLOPs:} $2BN_H S^2 = O(BN_H S^2)$

\subsection{Step 5: Softmax}

\paragraph{Operation:} \iconAux{S}
\paragraph{FLOPs:} $4BN_H S^2 = O(BN_H S^2)$

\subsection{Step 6: Attention $\times$ Value}

\paragraph{Operation:} \iconMatMul
\paragraph{FLOPs:} $2BS^2D = O(BS^2D)$ \textbf{(Second $O(S^2)$ bottleneck)}

\subsection{Steps 7-9: Concatenate, Output Projection, Dropout}

\paragraph{FLOPs:} $2BSD^2$ (output projection)

\subsection{Forward Pass Total Complexity}

\begin{table}[h]
\centering
\caption{Forward Pass FLOPs Summary}
\begin{tabular}{lc}
\toprule
\textbf{Component} & \textbf{FLOPs} \\
\midrule
Linear Projections & $8BSD^2$ \\
Attention Core & $4BS^2D$ \\
Other Operations & $O(BN_H S^2)$ \\
\midrule
\textbf{Total} & $8BSD^2 + 4BS^2D + O(BN_H S^2)$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Complexity Analysis:}
\begin{itemize}
  \item \textbf{Short sequences ($S \ll D$)}: $O(BSD^2)$ dominates (linear projections)
  \item \textbf{Long sequences ($S \gg D$)}: $O(BS^2D)$ dominates (attention)
  \item \textbf{Crossover}: $S \approx 2D$
\end{itemize}

\section{Backward Pass Analysis}
\label{sec:backward}

The backward pass receives gradient $\frac{\partial L}{\partial \mathbf{A}_{\text{out}}}$ from the loss function and computes gradients for all parameters and inputs. Generally, backward pass computation is approximately 2x that of forward pass.

\subsection{Key Observations}

\begin{enumerate}
  \item Each forward operation requires computing gradients for both inputs and weights
  \item Total backward FLOPs: $16BSD^2 + 8BS^2D + O(BN_H S^2)$
  \item \textbf{Ratio}: Backward is approximately 2x forward
\end{enumerate}

\subsection{Memory Considerations}

\begin{itemize}
  \item Must cache intermediate activations from forward pass
  \item Alternative: recompute activations during backward (gradient checkpointing)
  \item Trade-off: Memory vs computation time
\end{itemize}

\section{Complexity Summary and Optimization}
\label{sec:summary}

\subsection{Overall Complexity}

\begin{table}[h]
\centering
\caption{Multi-Head Attention Total Complexity}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{FLOPs} & \textbf{Memory} \\
\midrule
\textbf{Forward} & & \\
\quad Linear Projections & $8BSD^2$ & $4D^2 + 4BSD$ \\
\quad Attention Core & $4BS^2D$ & $BN_H S^2$ \\
\midrule
\textbf{Backward} & & \\
\quad Linear Projections & $16BSD^2$ & $4D^2$ \\
\quad Attention Core & $8BS^2D$ & $BN_H S^2$ \\
\midrule
\textbf{Total (Fwd+Bwd)} & $24BSD^2 + 12BS^2D$ & $8D^2 + BN_H S^2$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scaling Characteristics}

\subsubsection{Sequence Length $S$}

\textbf{Critical observation}: Attention has $O(S^2)$ complexity in both FLOPs and memory.

\textbf{Example}: $B=32, N_H=32, S=8192$, float32
\begin{equation}
\text{Memory}(\mathbf{A}) = 32 \times 32 \times 8192^2 \times 4 \text{ bytes} = 256 \text{ GB}
\end{equation}

This exceeds single GPU memory ($\sim$80GB)!

\subsection{Optimization Strategies}

\subsubsection{FlashAttention}

\textbf{Problem}: Standard attention requires $O(S^2)$ memory.

\textbf{Solution}: IO-aware attention using tiling and recomputation.

\textbf{Benefits}:
\begin{itemize}
  \item Memory: $O(BS \cdot D)$ instead of $O(BN_H S^2)$ \textbf{($S^2 \rightarrow S$ reduction)}
  \item Speed: 2-4x faster wall-clock time
  \item Exact: Same results as standard attention
\end{itemize}

\subsubsection{Multi-Query / Grouped-Query Attention}

\textbf{Problem}: KV cache dominates inference memory.

\textbf{Solution}:
\begin{itemize}
  \item \textbf{MQA}: All heads share single K, V ($N_H$ heads, 1 KV pair)
  \item \textbf{GQA}: Groups of heads share K, V ($N_H$ heads, $G$ KV pairs)
\end{itemize}

\textbf{Benefits}:
\begin{itemize}
  \item MQA: $32\times$ KV cache reduction
  \item GQA: $4\times$ reduction (with $G=8$)
  \item Quality: GQA $\approx$ standard $>$ MQA
\end{itemize}

\subsubsection{Gradient Checkpointing}

\textbf{Problem}: Must store all activations for backward pass.

\textbf{Solution}: Store only layer boundaries, recompute during backward.

\textbf{Trade-off}:
\begin{itemize}
  \item Memory: $O(L \cdot BSD) \rightarrow O(\sqrt{L} \cdot BSD)$
  \item Time: $\sim$33\% increase
\end{itemize}

\subsubsection{Mixed Precision Training}

\textbf{FP16/BF16 Benefits}:
\begin{itemize}
  \item Memory: 2x reduction
  \item Speed: 2-3x faster (with tensor cores)
  \item Stability: BF16 $>$ FP16
\end{itemize}

\subsection{Practical Recommendations}

\subsubsection{Training}

\begin{enumerate}
  \item Use gradient checkpointing for $L > 24$ layers
  \item Enable FlashAttention (always)
  \item Use BF16 mixed precision
  \item Maximize batch size to 90-95\% GPU memory
\end{enumerate}

\subsubsection{Inference}

\begin{enumerate}
  \item Use GQA or MQA for KV cache efficiency
  \item Apply INT8 quantization (2-4x speedup)
  \item Consider speculative decoding for latency
  \item Use sliding window for very long contexts
\end{enumerate}

\section{Conclusion}

This paper provided a systematic analysis of the Multi-Head Attention mechanism from a computational graph perspective. We explained the mathematical definitions and characteristics of each primitive operation, and quantitatively derived the computational and memory costs at each stage of both forward and backward passes.

\paragraph{Key Findings}:
\begin{enumerate}
  \item \textbf{Dual bottleneck}: Linear projections for short sequences, attention for long sequences
  \item \textbf{Memory bottleneck}: $O(S^2)$ attention scores, cumulative KV cache
  \item \textbf{Backward is 2x forward}: Due to computing both input and weight gradients
\end{enumerate}

\paragraph{Optimization Direction}:
\begin{itemize}
  \item FlashAttention (essential): $O(S^2) \rightarrow O(S)$ memory
  \item GQA/MQA: 4-32x KV cache reduction
  \item Mixed precision + fused kernels: 2-3x speedup
  \item Gradient checkpointing: $\sqrt{L}$ memory reduction
\end{itemize}

This analysis provides LLM researchers and engineers with the understanding needed to design efficient Transformer implementations.

\begin{thebibliography}{99}

\bibitem{vaswani2017attention}
Vaswani, A., et al. (2017). Attention is all you need. NeurIPS.

\bibitem{dao2022flashattention}
Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention. NeurIPS.

\bibitem{shazeer2019fast}
Shazeer, N. (2019). Fast transformer decoding: One write-head is all you need. arXiv:1911.02150.

\bibitem{ainslie2023gqa}
Ainslie, J., et al. (2023). GQA: Training Generalized Multi-Query Transformer Models. arXiv:2305.13245.

\end{thebibliography}

\end{document}
